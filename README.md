# Intelligent Document Processing and Analysis System

A comprehensive enterprise-grade solution for extracting, analyzing, and querying financial data from investment fund prospectuses. This system leverages advanced Large Language Models (LLMs) including Google Gemini 2.5 Flash Lite and Mistral Small, combined with Hybrid Retrieval-Augmented Generation (RAG) for deep document understanding.

## System Architecture

### Core Components
- **Backend**: Django REST Framework (DRF) serving as the orchestration layer.
- **Frontend**: React.js with Vite, providing a responsive interface for document management and analytics.
- **Database**: PostgreSQL with `pgvector` extension for storing structured relational data and high-dimensional vector embeddings.
- **AI Engine**: Hybrid integration of Google Gemini 2.5 Flash Lite and Mistral AI for OCR, data extraction, and semantic reasoning.

### Workflow
1. **Ingestion**: PDF documents are uploaded, validated, and securely stored.
2. **Preprocessing**: 
   - **Optimization**: Smart page segmentation identifies and isolates high-value pages (e.g., fee schedules, portfolio tables) to reduce context window usage.
   - **Hybrid OCR**: Routing logic selects between Text-PDF parsing and Mistral/Gemini vision capabilities for scanned documents.
3. **Extraction**: Structured financial data (NAV, fees, portfolio holdings) is extracted into normalized JSON schemas.
4. **Vectorization (RAG)**:
   - Full document text is segmented into semantic chunks using Mistral OCR for highest-fidelity markdown extraction.
   - 1024-dimensional embeddings are generated using Mistral (`mistral-embed-2312`).
   - Vectors are indexed in PostgreSQL using HNSW (Hierarchical Navigable Small World) graphs for sub-millisecond similarity search.
   - A `tsvector` GIN index is maintained in parallel for keyword (BM25-style) search.
5. **Analysis**: Users can query documents via a chat interface. The system retrieves relevant context (RAG) and combines it with structured database records to provide hallucination-free answers.

## Features

### Advanced Data Extraction
- **Multi-Model Support**: Toggle between Gemini 2.5 Flash Lite and Mistral Small/OCR models based on document complexity.
- **Structured Normalization**: Automatically standardizes varying prospectus formats into a unified schema:
  - Fund Identity (Name, Code, Management Company)
  - Fee Structures (Subscription, Redemption, Management, Switching)
  - Portfolio Holdings (Assets, Allocation percentages)
  - Historical Data (NAV History, Dividend Distributions)
- **Visual Grounding**: Generates bounding boxes for extracted fields, allowing users to visually verify data origins on the PDF.

### Retrieval-Augmented Generation (RAG)
- **Context-Aware Chat**: Specialized Q&A system capable of answering complex financial questions (e.g., "Explain the risk profile," "Compare fees with industry average").
- **Hybrid Search**: Reciprocal Rank Fusion (RRF) combines dense vector search (Mistral embeddings, cosine distance) with keyword search (PostgreSQL `tsvector` GIN index) for higher recall and precision.
- **Configurable Chat Provider**: RAG answers are generated by the provider set via `RAG_CHAT_PROVIDER` — defaults to **Ollama (qwen2.5:7b)** for fully local inference; also supports Gemini 2.5 Flash Lite and Mistral.
- **Dual-Source Reasoning**: The answering engine synthesizes information from:
  1. **Structured Data**: For precise queries (fees, codes, dates).
  2. **Hybrid-Retrieved Chunks**: For qualitative queries (investment strategy, risk factors).
- **Source Attribution**: Citations link responses back to specific pages and text chunks.

### Enterprise Features
- **Audit Logging**: Comprehensive tracking of all data modifications and user actions (`DocumentChangeLog`).
- **Versioning**: Track edits to extracted data with rollback capabilities.
- **Performance Monitoring**: Dashboard for tracking processing success rates, latency, and confidence scores.

## Technical Stack

### Backend
- **Framework**: Django 5.x, Django REST Framework
- **Database**: PostgreSQL 16 + `pgvector`
- **AI/ML**: 
  - **RAG Extraction (primary)**: Mistral OCR (`mistral-ocr-latest`) for highest-fidelity markdown
  - **OCR / Extraction**: Google Gemini 2.5 Flash Lite (`gemini-2.5-flash-lite`) via `google-genai` SDK
  - **OCR / Extraction (alt)**: Mistral Small (`mistral-small-latest`) + Mistral OCR (`mistral-ocr-latest`)
  - **Embeddings**: Mistral `mistral-embed-2312` (1024 dimensions)
  - **Reranker**: FlashRank `ms-marco-MiniLM-L-12-v2` (cross-encoder, optional)
  - **RAG Chat**: Ollama (`qwen2.5:7b`, default) / Gemini 2.5 Flash Lite / Mistral (configurable via `RAG_CHAT_PROVIDER`)
  - `langchain-text-splitters` for RAG chunking
  - `PyMuPDF` (Fitz) & `RapidOCR` for PDF manipulation and highlight snapping
- **Async Processing**: Python threading for non-blocking I/O

### Frontend
- **Framework**: React 18
- **Build Tool**: Vite
- **Styling**: Tailwind CSS
- **State Management**: React Hooks & Context
- **Visualization**: `recharts` for financial data, `react-pdf` for rendering

## Setup Instructions

### Prerequisites
- Python 3.13+ (Recommended)
- Node.js 18+
- PostgreSQL 15+ with `pgvector` extension installed
- [Ollama](https://ollama.com/) with the following models pulled:
  - `qwen2.5:7b` — default RAG chat
  - `llama3.1:8b` — RAGAS evaluation judge
  - `nomic-embed-text` — RAGAS evaluation embeddings
- API Keys for Google Gemini and/or Mistral AI

### Backend Configuration

1. **Clone and Navigate**:
   ```bash
   git clone <repository-url>
   cd backend
   ```

2. **Environment Setup**:
   ```bash
   python -m venv .venv
   # Windows:
   .venv\Scripts\activate
   # Linux/Mac:
   source .venv/bin/activate
   ```

3. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```
   *Note: Ensure C++ Build Tools are installed if compiling specific wheels is required.*

4. **Environment Variables**:
   Copy `.env.example` to `.env` and configure:
   ```env
   SECRET_KEY=your-secure-key
   DATABASE_URL=postgresql://user:pass@localhost:5432/db_name
   GEMINI_API_KEY=your_key
   MISTRAL_API_KEY=your_key
   DJANGO_DEBUG=False

   # RAG chat provider: ollama (default) | gemini | mistral
   RAG_CHAT_PROVIDER=ollama
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_MODEL=qwen2.5:7b
   ```

5. **Database Initialization**:
   ```bash
   python manage.py makemigrations
   python manage.py migrate
   ```
   *This will enable the `vector` extension and create schemas.*

6. **Launch Server**:
   ```bash
   python manage.py runserver
   ```

### Frontend Configuration

1. **Navigate**:
   ```bash
   cd frontend
   ```

2. **Install Packages**:
   ```bash
   npm install
   ```

3. **Launch Client**:
   ```bash
   npm run dev
   ```

## API Documentation

### Core Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/documents/` | Upload document (triggers async extraction) |
| `GET` | `/api/documents/{id}/` | Retrieve metadata and extracted JSON |
| `PATCH` | `/api/documents/{id}/` | Manually correct extracted data |
| `POST` | `/api/documents/{id}/chat/` | Send RAG-based query to document context |
| `GET` | `/api/documents/{id}/preview-page/{page}/` | Get rendered page with bounding box overlays |
| `GET` | `/api/documents/{id}/change_logs/` | View audit trail of edits |

## RAG Evaluation & Optimization (RAGAS)

To ensure the reliability of financial data retrieval, this project implements a quantitative evaluation pipeline using the **RAGAS** framework. Instead of relying on qualitative "feeling," we measure the system's performance using a test set of complex Vietnamese financial queries.

### Performance Evolution

We benchmarked the retrieval system across three generations of the pipeline.

> ⚠️ **Note on comparability:** v1 and v2 were evaluated on **different question sets**, so scores are not directly comparable. Additionally, v1 used **semantic (vector) search only**; hybrid keyword search was introduced in v2. The table below reflects directional progress across pipeline generations, not a controlled A/B test.

| Metric | Baseline (Standard OCR) | v1 (Semantic Search only) | **v2 (Hybrid Search + FlashRank)** |
| :--- | :---: | :---: | :---: |
| **Faithfulness** | 0.3464 | 0.6543 | **0.7135** |
| **Answer Relevancy** | 0.3777 | 0.2800 | **0.6762** |
| **Context Precision** | 0.1217 | 0.6573 | **0.7126** |
| **Context Recall** | 0.2812 | 0.9630 | **0.8250** |

> **Evaluation setup — v2:** Judge LLM — local Ollama `llama3.1:8b` (JSON mode, `num_ctx=8192`); Embeddings — `nomic-embed-text`; Context — top 5 chunks × 1200 chars max; evaluated on a separate, independently generated question set.

### Key Optimization Strategies implemented:

1.  **Structured Markdown Extraction**: Switched from raw text extraction to **Mistral OCR**, which preserves document structure (tables, headers). This resolved "infinite loop" hallucinations in the vector space and allowed the embedding model to correctly interpret complex fee schedules.
2.  **Noise Reduction Pipeline**: Implemented a heuristic cleaning layer to strip repetitive headers (e.g., "ỦY BAN CHỨNG KHOÁN") and footers that were poisoning the vector search results, leading to the massive jump in **Context Precision**.
3.  **Hybrid Search (RRF)**: Replaced pure vector retrieval with Reciprocal Rank Fusion of dense embeddings (Mistral `mistral-embed-2312`) and sparse keyword search (PostgreSQL `tsvector` GIN index). This boosted Context Recall significantly on queries containing specific fund codes, fee values, and proper nouns.
4.  **FlashRank Cross-Encoder Reranking**: After the initial hybrid retrieval (15 candidates), a **FlashRank `ms-marco-MiniLM-L-12-v2`** cross-encoder reranks all candidates against the query and selects the top 5. This directly cut hallucinations by removing marginally-related chunks that passed the vector distance threshold but were not genuinely relevant — raising Faithfulness from 0.65 to **0.71**. Context Precision also improved as the most topically precise chunks were consistently promoted to the top of the context window. Crucially, ASCII normalization was applied consistently: `content_ascii` (via `unidecode()`) is stored per chunk, and both the `search_vector` GIN index and live queries use the same `config='simple'` normalization — ensuring Vietnamese diacritic variants like `Quỹ` and `Quy` are matched correctly.
5.  **Local Evaluation Pipeline**: Transitioned the "Judge" LLM to a local **Ollama `llama3.1:8b`** instance (JSON mode, `num_ctx=8192`) with `nomic-embed-text` for embedding scoring. This bypassed API rate limits (HTTP 429) and allowed for cost-effective, intensive regression testing. Sequential execution (`max_workers=1`) prevents timeouts on resource-constrained machines.
## Future Improvements

The following roadmap outlines planned enhancements to elevate the system's capabilities:

1.  **Distributed Task Queue**: Migrate from Python threading to Celery/Redis for robust, horizontally scalable background processing.
2.  **Advanced RAG Techniques**:
    -   **Multi-Document Querying**: Allow users to ask questions across the entire document corpus (e.g., "Compare the management fees of all Balanced Funds").
3.  **Authentication & Multi-Tenancy**: Implement OAuth2/JWT authentication to support multiple organizations with isolated data context.
4.  **Feedback Loop**: Implement User-in-the-Loop (HITL) fine-tuning, where manual corrections to extracted data are fed back to improve future prompts or fine-tune smaller models.
5.  **CI/CD Pipeline**: GitHub Actions workflows for automated testing, linting, and containerized deployment (Docker/Kubernetes).

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
